{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A movie Recommendation System Using MovieLens Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Overview**\n",
    "\n",
    "This project focuses on analyzing movie ratings from the MovieLens dataset to provide tailored movie recommendations. It combines collaborative filtering and content-based filtering, to identify the top 5 movies most suited to a user's interests. This dual approach enhances the recommendation accuracy by integrating both user-driven insights and movie-specific characteristics, creating a robust, personalized viewing experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Understanding**\n",
    "\n",
    "In today’s digital age, online streaming platforms such as Netflix, Showmax,HBO,provide a vast library of movies across various genres, which users can stream on devices like smartphones, tablets, smart TVs, and computers. These platforms generate vast amounts of data on user interactions, including viewing habits, preferences, and ratings. However, the challenge lies in leveraging this data to enhance user engagement by providing personalized movie recommendations that align with individual tastes. \n",
    "\n",
    "Personalization has a proven impact on engagement; for example, [75% of what people watch on Netflix](https://litslink.com/blog/all-about-netflix-artificial-intelligence-the-truth-behind-personalized-content) comes from its personalized recommendations, highlighting the importance of robust recommendation algorithms in guiding users toward their next favorite show or movie. The goal of this project is to improve user satisfaction and retention by suggesting the top 5 movies a user is likely to enjoy, utilizing a blend of collaborative and content-based filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**\n",
    "\n",
    "With the increasing volume of movies available on streaming platforms, users often experience choice overload, making it difficult to discover content that matches their preferences. This reduces user satisfaction and engagement, negatively impacting retention rates for content providers. The challenge is to develop a personalized recommendation system that can accurately predict and suggest movies tailored to individual users' tastes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "1. To identify the top 10 most popular genres based on user ratings.\n",
    "2. To identify if there is a relationship between movie genres and ratings.\n",
    "3. To examine user-generated tags to identify popular keywords associated with genres and movie themes.\n",
    "4. To build a recommendation system that suggests the top 5 rated movies to a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Understanding**\n",
    "\n",
    "The [dataset](https://grouplens.org/datasets/movielens/latest/) for the analysis is from the GroupLens research lab at the University of Minnesota. It entails 100,000 movie ratings. It entails different csv files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import f_oneway\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from surprise import SVD,Reader, Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the csv files\n",
    "from Functions import load_data\n",
    "\n",
    "#Defining the csv file paths \n",
    "movie_file_path = \"Data/movies.csv\"\n",
    "rating_file_path= \"Data/ratings.csv\"\n",
    "tags_file_path= \"Data/tags.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Importing function for loading data\n",
    "from Functions import explore_data\n",
    "\n",
    "#Loading and inspecting the movie csv file\n",
    "movie_df = pd.read_csv(movie_file_path)  \n",
    "explore_data(movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and inspecting the movie ratings csv file\n",
    "rating_df = pd.read_csv(rating_file_path)  \n",
    "explore_data(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and inspecting movie tags csv\n",
    "tags_df = pd.read_csv(tags_file_path)  \n",
    "explore_data(tags_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "This section will involve checking the following about the dataset.\n",
    "- Accuracy\n",
    "- Validity\n",
    "- Completeness\n",
    "- Uniformity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "from Functions import check_and_remove_duplicates\n",
    "# Checking and removing duplicates in movies dataset\n",
    "movie_df = check_and_remove_duplicates(movie_df)\n",
    "\n",
    "# Checking and removing duplicates in ratings dataset\n",
    "rating_df = check_and_remove_duplicates(rating_df)\n",
    "\n",
    "# Checking and removing duplicates in tags dataset\n",
    "tags_df = check_and_remove_duplicates(tags_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "After running the check on all three datasets (movies, ratings, and tags), we confirm that no duplicate rows were found in any of the datasets. Therefore, the data is clean in this regard and requires no further action to remove duplicates, ensuring the integrity of the subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outliers in the 'rating' column of the ratings dataset\n",
    "from Functions import visualize_outliers_with_boxplot\n",
    "visualize_outliers_with_boxplot(rating_df, 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The boxplot of the rating column shows that most ratings fall between 3.0 and 4.5, indicating a general preference for higher-rated movies, with a few low outliers reflecting strong dislikes.These outliers are retained because they provide meaningful insights into user behavior, particularly reflecting strong negative opinions about certain movies. Low ratings are critical for understanding user dislikes, which is essential for preventing the recommendation system from suggesting unsuitable movies. By maintaining these outliers, the recommendation system can better capture the diversity of user sentiments, leading to more robust and personalized recommendations that accurately represent the full spectrum of user preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Validity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for consistency in datatypes\n",
    "#Converting Timestamp into DATE format\n",
    "\n",
    "#Converting the timestamp into date format\n",
    "rating_df['timestamp'] = pd.to_datetime(rating_df['timestamp'])\n",
    "tags_df['timestamp']= pd.to_datetime(tags_df['timestamp'])  \n",
    "\n",
    "\n",
    "#Extracting the date\n",
    "\n",
    "rating_df['timestamp'] = rating_df['timestamp'].dt.date\n",
    "tags_df['timestamp']=tags_df['timestamp'].dt.date\n",
    "tags_df['timestamp']\n",
    "rating_df['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for unique values in the date column for the rating csv\n",
    "rating_df['timestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for unique values in the date column for the tags csv\n",
    "tags_df['timestamp'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the timestamp column only indicates the time when users rated a movie and all values fall within the same year (1970), it does not add valuable information to our analysis. Therefore, we will drop it from the dataset to streamline our data and focus on more relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop irrelevant columns\n",
    "#Dropping the timestamp column\n",
    "rating_df=rating_df.drop('timestamp',axis=1)\n",
    "tags_df=tags_df.drop('timestamp',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm the column names after dropping timestamp\n",
    "rating_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm the column names after dropping the timestamp column\n",
    "tags_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for column names\n",
    "\"\"\" The column names are consistent in naming for all files. They are also all in lower case.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Completeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "\"\"\"\n",
    "The data presented not to have any missing values, so no imputation was done.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Consistency/Uniformity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column names capitalization\n",
    "\"\"\"\n",
    "All the columns displayed consistency in their naming and were all in lower case.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if ratings lie within the same range\n",
    "\"\"\"\n",
    "All the movie ratings were in the same range, ranging from 0.5 to 5 star rating, with no missing value as well.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the movie titles from the years\n",
    "movie_df[\"Release_year\"] = movie_df[\"title\"].apply( lambda x: x[-5:-1] if x[-5:-1].isdigit() else \"0000\")\n",
    "movie_df[\"Release_year\"] = movie_df[\"Release_year\"].astype(\"int64\")\n",
    "movie_df[\"Release_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df[\"genres\"].unique()[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most movies seem to be a **combination of multiple genres** hence why there are so so many unique combinations.\n",
    "\n",
    "Let's seperate these combinations into individual genres and get more granular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the combinations into individual genres by removing the \",\" seperators\n",
    "movie_df['genres'] = movie_df['genres'].str.split('|')\n",
    "\n",
    "# create a new Exploded dataframe with the list of genres into individual rows\n",
    "movie_df= movie_df.explode('genres').reset_index(drop=True)\n",
    "\n",
    "\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genres column is now made up of lists of individual genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging the movie_df with rating_df** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=pd.merge(rating_df,movie_df, on='movieId')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_data(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for the Distribution of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_df = merged_df[[\"rating\", \"movieId\", \"userId\"]]\n",
    "\n",
    "# Create 4x4 grid of subplots\n",
    "fig, axes = plt.subplots(3, figsize=(10,10))\n",
    "\n",
    "# Flatten axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Looping through each column and plotting in the respective subplot\n",
    "for i, col in enumerate(numerical_cols_df):\n",
    "    sns.histplot(x=rating_df[col], ax=axes[i], kde=True)  \n",
    "    axes[i].set_title(f'Distribution of {col}')  \n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The analysis reveals that movie ratings are generally skewed towards higher values, with most users giving positive feedback. Although there are a few instances of very low ratings, these are exceptions rather than the norm. \n",
    "\n",
    "Regarding movie IDs, the distribution shows a heavy right-skew, indicating that while many movies receive few ratings, a smaller set of movies attracts a large number of ratings. \n",
    "\n",
    "Lastly, the distribution of user IDs is relatively uniform, suggesting that a moderate number of users contribute a similar amount of ratings, although some users may rate more frequently than others.\n",
    "\n",
    "To address the skewness in the data, we will apply MinMax scaling during the modeling phase.Since we decided to retain the outliers in our analysis, MinMax scaling will normalize the data to a fixed range while preserving the relative relationships between the features, including the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. To identify the top 10 most popular genres based on the number of user ratings.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by genres and calculate the total number of ratings\n",
    "genre_ratings = merged_df.groupby('genres').agg(\n",
    "    Rating_Count=('rating', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Sort genres by the number of ratings and select the top 10\n",
    "top_genres = genre_ratings.sort_values(by='Rating_Count', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 genres\n",
    "print(top_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 10 genres\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Rating_Count', y='genres', data=top_genres, palette='viridis')\n",
    "plt.title('Top 10 Most Popular Genres Based on User Ratings')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Genres')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The graph displays the top 10 most-rated genres, with Drama having the highest number of user ratings, followed by Comedy and Action. This sheds light on the kind of genres which were highly watched by different users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. To Identify the top 10 Genres with the highest ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group by genre and calculate the average rating for each genre\n",
    "top_10_genres_avg_rating =merged_df.groupby('genres')['rating'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 genres\n",
    "top_10_genres = top_10_genres_avg_rating.head(10)\n",
    "\n",
    "# Plotting the top 10 genres by average rating\n",
    "plt.figure(figsize=(10,6))\n",
    "top_10_genres.plot(kind='bar', color='skyblue')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Top 10 Genres by Average Rating', fontsize=16)\n",
    "plt.xlabel('Genre', fontsize=14)\n",
    "plt.ylabel('Average Rating', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The graph presents the genres which were highly rated, with Film-Noir presenting to have the highest rating, followed by war then documentary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. To Identify the distribution of movie ratings per user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the average ratings per user\n",
    "ratings_per_user = merged_df.groupby('userId')['rating'].mean()\n",
    "\n",
    "# Create a histogram of ratings per user\n",
    "plt.figure(figsize=(12, 6))\n",
    "ratings_per_user.plot(kind='hist', bins=50, color='skyblue', edgecolor='black') \n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Distribution of average Ratings per User')\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Frequency of Users')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The distribution indicates that the majority of movie ratings fall within the 2 to 4.5 star range, suggesting that most users tend to rate movies positively. However, the presence of a few low ratings below 2 stars highlights that there are still some users who express strong dissatisfaction, potentially due to varying expectations or specific negative experiences with certain films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.To identify the trend of movies' production by decade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of movies by decade\n",
    "movie_df['Decade'] = (movie_df['Release_year'] // 10) * 10\n",
    "movies_by_decade = movie_df.groupby('Decade').size()\n",
    "\n",
    "# Extract decades and counts\n",
    "decades = movies_by_decade.index\n",
    "counts = movies_by_decade.values\n",
    "\n",
    "# Create the line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=decades.astype(str), y=counts, marker='o', color='blue') \n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Trend of Movie production by Decade', fontsize=16)\n",
    "plt.xlabel('Decade', fontsize=14)\n",
    "plt.ylabel('Number of Movies', fontsize=14) \n",
    "\n",
    "# Customize x-axis ticks\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The graph presents that movie production remained slow in the 1900s, but saw a significant increase starting in the 1970s. This upward trajectory can be attributed to technological advancements, higher demand for films, and greater investment in the film industry. However, from the 2000s onward, production is seen to fluctuate, likely due to changes in market dynamics, including the rise of digital streaming platforms, evolving consumer preferences, and increased competition within the entertainment industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. To examine the number of ratings per movie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set a threshold to focus on movies with a reasonable number of ratings\n",
    "# ratings_threshold = 50  \n",
    "\n",
    "# # Filter ratings per movie to focus on movies with less than the threshold\n",
    "# filtered_ratings = ratings_per_movie[ratings_per_movie <= ratings_threshold]\n",
    "\n",
    "# # Plot the histogram for the filtered data\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.histplot(filtered_ratings, bins=30, kde=False, color='skyblue')\n",
    "# plt.title('Distribution of Number of Ratings Per Movie (Filtered)', fontsize=16)\n",
    "# plt.xlabel('Number of Ratings', fontsize=14)\n",
    "# plt.ylabel('Count of Movies', fontsize=14)\n",
    "# plt.xticks(fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. To identify if there is a relationship between movie genres and ratings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis (H₀): There is no relationship between movie genres and ratings.\n",
    "\n",
    "Alternative Hypothesis (H₁): There is a significant relationship between movie genres and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_ratings = merged_df.groupby('genres')['rating'].apply(list)\n",
    "\n",
    "# Perform one-way ANOVA (if assumptions are satisfied)\n",
    "anova_result = f_oneway(*genre_ratings)\n",
    "print('ANOVA result:', anova_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "F-statistic: 176.87 suggests a large difference between the mean ratings of the different genres.\n",
    "\n",
    "Since the p-value is less than 0.05, we reject the null hypothesis. This means that there is a significant difference between the average ratings of at least some of the movie genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examining rating distributions by genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='genres', y='rating', data=merged_df, palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Ratings by Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Since the hypothesis testing highlighted that there is a significant difference in ratings by genre, the above box plots displays how these ratings vary by genre. With some genre ratings ranging from 3-4 star ratings , others 2.5 to 4.5 star ratings while others 3.5 to 4.5 star rating.This disparity reveals that, certain genres tend to receive more consistent ratings, while others experience greater variability, possibly reflecting differing audience expectations, preferences, or subjective opinions about the films within each genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. To examine user-generated tags to identify popular keywords associated with movie themes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stop words list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# converting the tags to lowercase and removing non-alphabetic characters\n",
    "tags_df['tag'] = tags_df['tag'].str.lower().apply(lambda x: re.sub(r'[^a-z\\s]', '', x))\n",
    "\n",
    "# Tokenize the tags, remove stop words, and apply lemmatization\n",
    "tags_df['processed_tag'] = tags_df['tag'].apply(lambda x: [\n",
    "    lemmatizer.lemmatize(word) for word in word_tokenize(x) if word not in stop_words\n",
    "])\n",
    "\n",
    "# Flatten the list of lists to get all words in one list\n",
    "all_words = [word for tag in tags_df['processed_tag'] for word in tag]\n",
    "\n",
    "# Get word frequency\n",
    "word_counts = pd.Series(all_words).value_counts().reset_index()\n",
    "word_counts.columns = ['word', 'count']\n",
    "\n",
    "# Display the top 10 most frequent words\n",
    "top_10_words = word_counts.head(10)\n",
    "\n",
    "# Plotting the top 10 words\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_words['word'], top_10_words['count'], color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Most Frequent Words in Movie Tags')\n",
    "plt.show()\n",
    "\n",
    "# Print the top 10 words\n",
    "print(top_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The most frequent words highlights on users preferences.\n",
    "\n",
    "- Netflix being on the top would highly imply that most movies were watched from netflix.\n",
    "\n",
    "- Queue would indicate that most users usually tag or add movies to their watchlist.\n",
    "\n",
    "- Word like Ending would suggest that most users are particularly focused on the conclusion of movies implying that a movie's ending is a key feature for most films.\n",
    "\n",
    "- The word Movie appears commonly to reflect that most users tag films as 'movies' compared to the 'series' term. This implies that movie is frequently watched compared to series.\n",
    "\n",
    "- Other words such as funny, dark and comedy highlights on the frequently watched themes, which would be key in offering a movie recommendation.\n",
    "\n",
    "- Sci-Fi, space and Atmospheric implies that there is also a good chunk of users who are drawn to watching movies related to science fiction genres or movies with strong mood setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. To build a recommendation system that suggests the top 5 rated movies to a user.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collaborative filtering\n",
    "#-memorybased-knn\n",
    "#model-based-svd-tuning\n",
    "\n",
    "#1. Identify the users/items-user based collaborative filtering/Memory(use the entire dataset)\n",
    "#2. Dealing sparsity\n",
    "#Matrix Factorization using SVD\n",
    "#3. Dealing with coldstart problem-content based filtering\n",
    "#4. Scaling the data-apply pipelines\n",
    "#5. Building the model-KNN,-specify the similarity metric(cosine,pearson,Jaccard)\n",
    "#6.SVD-tune\n",
    "\n",
    "#Data preprocessing\n",
    "#Scaling data\n",
    "#Encoding categorical values\n",
    "#Deal with sparsity i.e Remove users or items with too few interactions.\n",
    "#Apply dimensionality reduction methods or matrix factorization (e.g., SVD)\n",
    "# Address coldstart problem-\n",
    "#Convert the data into a format suitable for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling our data\n",
    "scaler=StandardScaler()\n",
    "\n",
    "#fit scaled ratings\n",
    "rating_df['scaled_rating']=scaler.fit_transform(rating_df[['rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied StandardScaler to our data to handle outliers in the rating column, as we chose to work with them. This approach is particularly helpful given that the algorithms used in our model, KNN and SVD, are sensitive to outliers.The approach will reduce the effect of outliers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming data to be compatible with surprise library\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(rating_df[['userId', 'movieId', 'rating']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for the number of users and items in our dataset\n",
    "dataset = data.build_full_trainset()\n",
    "print('Number of users: ', dataset.n_users, '\\n')\n",
    "print('Number of items: ', dataset.n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data presents to have fewer users compared to the number of movies, which creates a sparse matrix. The below code will inspect the level of sparsity for our data. Then follow up to apply either collaborative or content-based filtering based on the level of our data sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheking for level of sparsity for our data\n",
    "def create_X(df):\n",
    "    #Identify the unique users and movies in our data\n",
    "    num_users = df[\"userId\"].nunique()\n",
    "    num_movies = df[\"movieId\"].nunique()\n",
    "    \n",
    "    #Create mappings for user ids and movie ids \n",
    "    user_mapper = {user_id: i for i, user_id in enumerate(df[\"userId\"].unique())}\n",
    "    movie_mapper = {movie_id: i for i, movie_id in enumerate(df[\"movieId\"].unique())}\n",
    "    \n",
    "    #Inverse mappings for users and movies IDs\n",
    "\n",
    "    user_inv_mapper = {v: k for k, v in user_mapper.items()}\n",
    "    movie_inv_mapper = {v: k for k, v in movie_mapper.items()}\n",
    "    \n",
    "    #Map users and Movie IDs to indices\n",
    "\n",
    "    user_idx = df[\"userId\"].map(user_mapper)\n",
    "    movie_idx = df[\"movieId\"].map(movie_mapper)\n",
    "    \n",
    "    #Creates sparse matrix\n",
    "    X = csr_matrix((df[\"rating\"].values, (movie_idx, user_idx)), shape=(num_movies, num_users))\n",
    "    \n",
    "    #Calculating sparsity for our data\n",
    "\n",
    "    sparsity = 1 - (X.nnz / (X.shape[0] * X.shape[1]))\n",
    "\n",
    "    return X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper, sparsity\n",
    "\n",
    "#Display sparsity value for our data\n",
    "X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper, sparsity = create_X(rating_df)\n",
    "sparsity_value = 100 - (sparsity * 100) \n",
    "print(f\"Matrix sparsity: {sparsity_value:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sparsity value is above 1 meaning that our data is highly sparsed, which is typical in recommendation systems, as users generally rate only a small subset of movies they’ve watched. Therefore, to address this, we will consider collaborative filtering in building our recommendation system.This is because collaborative filtering leverages patterns in user-item interactions, even when most interactions are missing, by predicting missing ratings based on similarities between users or items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Gridsearch for SVD Model\n",
    "param_grid = {\n",
    "    'n_factors': [20, 50, 100],        # Number of latent factors\n",
    "    'reg_all': [0.02, 0.05, 0.1],      # Regularization term\n",
    "    'lr_all': [0.002, 0.005, 0.01],    # Learning rate\n",
    "    'n_epochs': [20, 30, 50]           # Number of epochs\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the SVD algorithm\n",
    "gs = GridSearchCV(SVD, param_grid,cv=5, n_jobs=-1)\n",
    "# Fit the GridSearchCV\n",
    "gs.fit(data)\n",
    "\n",
    "# Get the best parameters and their corresponding score\n",
    "print(\"Best RMSE score:\", gs.best_score)\n",
    "print(\"Best parameters:\", gs.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gridsearch results presents the best RMSE score to be 0.8501 which was achieved with 100 latent factors, a regularization term of 0.1, a learning rate of 0.01, and 50 epochs, while the best MAE score of 0.6502 came from 100 latent factors, a regularization term of 0.1, a learning rate of 0.01, and 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validating with KNNBasic\n",
    "knn_basic = KNNBasic(sim_options={'name':'pearson', 'user_based':True})\n",
    "cv_knn_basic = cross_validate(knn_basic, data, n_jobs=-1)\n",
    "for i in cv_knn_basic.items():\n",
    "    print(i)\n",
    "print('-----------------------')\n",
    "print(np.mean(cv_knn_basic['test_rmse']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test RMSE values range from 0.964 to 0.977, indicating consistent performance, while the test MAE values range from 0.743 to 0.754, reflecting a small margin of error. The model's fit time averages around 2 seconds, and the test time is approximately 4 seconds, showing relatively quick computation. The overall mean RMSE is 0.9716, suggesting stable performance across different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validating with KNNBaseline\n",
    "knn_baseline = KNNBaseline(sim_options={'name':'pearson', 'user_based':True})\n",
    "cv_knn_baseline = cross_validate(knn_baseline,data)\n",
    "for i in cv_knn_baseline.items():\n",
    "    print(i)\n",
    "\n",
    "np.mean(cv_knn_baseline['test_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test RMSE values range from 0.872 to 0.88, indicating consistent performance, while the test MAE values range from 0.66 to 0.67, reflecting minimal prediction error. The model's fit time averages around 1.30 seconds, and the test time is approximately 2.5 seconds, suggesting efficient computation. The overall mean RMSE is 0.8764, showing stable and reliable performance across different folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall Interpretation for the models**\n",
    "\n",
    "Comparing the Root Mean Squared Error(RMSE) for the above models, SVD presents to have the least rmse,signaling that it makes the most accurate predictions about users' ratings. RMSE measures the square root of the average squared differences between the predicted and actual ratings, so a lower value indicates that the model's predictions are closer to the true ratings. This suggests that SVD is better at capturing underlying patterns and preferences in the data compared to other models, making it the most suitable choice for building our recommendation system. Additionally, SVD works by breaking down the sparse matrix of user-item interactions into smaller, dense matrices (latent factors) that capture these hidden patterns, helping to predict missing ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making Recommendations using Collaborative filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and predicting the model\n",
    "svd = SVD(n_factors = 100, reg_all = 0.1, lr_all = 0.01, \n",
    "          n_epochs = 50)\n",
    "svd.fit(dataset)\n",
    "\n",
    "svd.predict(1,673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Checking for cold-start problems(collaborative filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_user_id = 900  # Initialize global user ID\n",
    "\n",
    "def movie_rater(movie_df):\n",
    "    global current_user_id  # Access the global variable\n",
    "    rating_list = []\n",
    "\n",
    "    # Prompt the user to input the number of movies to rate\n",
    "    num = int(input(\"Enter the number of movies you want to rate:\\n\").strip())\n",
    "\n",
    "    # Prompt the user to input a genre (optional)\n",
    "    genre = input(\"Enter a genre to filter movies (or press Enter to skip):\\n\").strip()\n",
    "    genre = genre if genre else None  # Set to None if the input is empty\n",
    "\n",
    "    movies_rated = 0  # Counter for the number of movies successfully rated\n",
    "\n",
    "    while movies_rated < num:\n",
    "        # Select a random movie, filtered by genre if specified\n",
    "        # lowercase genres in the DataFrame for case-insensitive matching\n",
    "        movie = movie_df[movie_df['genres'].str.lower().str.contains(genre, na=False)].sample(1) if genre else movie_df.sample(1)\n",
    "        print(movie)\n",
    "\n",
    "        # Prompt the user to rate the movie\n",
    "        rating = input('Rate this movie (1-5) or press \"n\" if you have not seen it:\\n').strip().lower()\n",
    "        print(f\"You rated this movie: {rating}\")\n",
    "        if rating == 'n':\n",
    "            print(\"Not Rated\")\n",
    "            continue  # Skip to the next movie if the user hasn't seen this one\n",
    "\n",
    "        # Add the user's rating to the list\n",
    "        rating_list.append({\n",
    "            'userId': current_user_id,\n",
    "            'movieId': movie['movieId'].values[0],\n",
    "            'rating': float(rating)\n",
    "        })\n",
    "        movies_rated += 1  # Increment the count of successfully rated movies\n",
    "\n",
    "    current_user_id += 1  # Increment the user ID for the next user\n",
    "    return rating_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = movie_rater(movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_ratings, movie_df, svd, genre=None, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommends movies for a user based on the movies they have rated.\n",
    "\n",
    "    Parameters:\n",
    "    - user_ratings: List of dictionaries with keys 'userId', 'movieId', and 'rating'.\n",
    "    - movie_df: DataFrame containing movie details with columns 'movieId' and 'title'.\n",
    "    - svd: Trained SVD model for making predictions.\n",
    "    - genre: (Optional) Genre to filter the movie recommendations by.\n",
    "    - num_recommendations: Number of movies to recommend (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    - List of recommended movies (titles).\n",
    "    \"\"\"\n",
    "    # Get the user ID from the ratings\n",
    "    user_id = user_ratings[0]['userId'] if user_ratings else None\n",
    "    if not user_id:\n",
    "        print(\"No user ratings provided.\")\n",
    "        return []\n",
    "\n",
    "    # Get the list of movies the user has already rated\n",
    "    rated_movie_ids = [rating['movieId'] for rating in user_ratings]\n",
    "    \n",
    "    # Convert the new ratings to a DataFrame\n",
    "    new_ratings_df = pd.DataFrame(user_ratings)\n",
    "\n",
    "    # Combine the new ratings with the existing dataset (assuming 'existing_ratings' is the original dataset)\n",
    "    updated_ratings = pd.concat([rating_df, new_ratings_df])\n",
    "\n",
    "    # Prepare the data for Surprise\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    surprise_data = Dataset.load_from_df(updated_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "    # refitting the model on the new dataframe\n",
    "    trainset_2 = surprise_data.build_full_trainset()\n",
    "    svd.fit(trainset_2)\n",
    "    \n",
    "    # Find movies the user hasn't rated and drop duplicates due to the exploded \"genres\" column\n",
    "    unrated_movies = movie_df[~movie_df['movieId'].isin(rated_movie_ids)].drop_duplicates(subset='movieId')\n",
    "    \n",
    "    # If genre filter is provided, apply it\n",
    "    if genre:\n",
    "        genre = genre.lower()\n",
    "        # Ensure the genre is part of any of the movie's genres (case-insensitive)\n",
    "        unrated_movies = unrated_movies[unrated_movies['genres'].str.contains(genre, na=False, case=False)]\n",
    "\n",
    "    # Predict ratings for unrated movies\n",
    "    unrated_movies['predicted_rating'] = unrated_movies['movieId'].apply(\n",
    "        lambda movie_id: svd.predict(user_id, movie_id).est\n",
    "    )\n",
    "\n",
    "    # Sort movies by predicted rating in descending order\n",
    "    recommendations = (\n",
    "        unrated_movies.sort_values(by='predicted_rating', ascending=False)\n",
    "        .head(num_recommendations)\n",
    "    )\n",
    "    \n",
    "    # Explanation message\n",
    "    print(\n",
    "        f\"Based on the {len(user_ratings)} movies you've rated, \"\n",
    "        \"here are some recommendations tailored to your preferences. \"\n",
    "        \"These movies have high predicted ratings, suggesting you might enjoy them!\"\n",
    "    )\n",
    "    print(\"Recommended Movies:\")\n",
    "\n",
    "\n",
    "    # Return the recommended movie titles\n",
    "    return recommendations[['title','predicted_rating']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print recommended movies\n",
    "recommendations = recommend_movies(user_ratings, movie_df, svd, num_recommendations=5)\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is not capturing enough diversity in the recommendations. This could be because:\n",
    "\n",
    "Hybrid Approach: Combine collaborative filtering (SVD) with content-based filtering (e.g., using genre similarity or movie features). This could allow for more diverse recommendations based on both user preferences and movie characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "#Use genres instead of tags\n",
    "# Sample DataFrame: Movie titles and their tags\n",
    "movies_df = pd.DataFrame({\n",
    "    'movieId': [1, 2, 3, 4],\n",
    "    'title': ['Movie A', 'Movie B', 'Movie C', 'Movie D'],\n",
    "    'tags': ['action adventure', 'comedy romance', 'action thriller', 'romance drama']\n",
    "})\n",
    "\n",
    "# Vectorize the tags using TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies_df['tags'])\n",
    "\n",
    "# Calculate the cosine similarity between all movie pairs\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Function to recommend movies based on similarity score\n",
    "def recommend_movies(movie_title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = movies_df.index[movies_df['title'] == movie_title].tolist()[0]\n",
    "    \n",
    "    # Get pairwise similarity scores for that movie with all other movies\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    # Sort the movies based on similarity score\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the indices of the top 3 most similar movies\n",
    "    sim_scores = sim_scores[1:4]  # Skip the first movie as it's the same as the input movie\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Return the top 3 most similar movies\n",
    "    return movies_df['title'].iloc[movie_indices]\n",
    "\n",
    "# Example: Recommend movies similar to 'Movie A'\n",
    "recommended_movies = recommend_movies('Movie A')\n",
    "print(recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing function for text preprocesing\n",
    "from Functions import preprocess_text\n",
    "\n",
    "# Combine relevant features into a single column\n",
    "merged_df[\"content\"] =merged_df[\"genres\"] + \" \" + merged_df[\"title\"] + \" \" + merged_df[\"Release_year\"].astype(str)\n",
    "\n",
    "#Apply preprocessing to the combined features\n",
    "merged_df[\"content\"] =merged_df[\"content\"].apply(preprocess_text)\n",
    "\n",
    "#Apply tf-idf to assign weights to each word\n",
    "tfidf = TfidfVectorizer(stop_words='english',max_features=1000, min_df=2)  \n",
    "tfidf_matrix = tfidf.fit_transform(merged_df[\"content\"])\n",
    "\n",
    "#Compute similarity score\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "#Get top 5 most similar movies for a given movie index\n",
    "movie_index = 0  \n",
    "similar_movies = list(enumerate(cosine_sim[movie_index]))\n",
    "similar_movies = sorted(similar_movies, key=lambda x: x[1], reverse=True)\n",
    "top_5_movies = similar_movies[1:6]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Filtering Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "1. Rashevskaya, A. (2024, April 12). All about Netflix artificial intelligence: The truth behind personalized content. Litslink. https://litslink.com/blog/all-about-netflix-artificial-intelligence-the-truth-behind-personalized-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
